{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!pip install split_folders\n",
    "import random\n",
    "import splitfolders\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.keras.applications import VGG16,VGG19, Xception, MobileNetV2, InceptionV3, ResNet50\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Download and save detection model\n",
    "def download_and_save_detection_model(model_url, save_path):\n",
    "    model = hub.load(model_url)\n",
    "    tf.saved_model.save(model, save_path)\n",
    "\n",
    "# Load detection model from file\n",
    "def load_detection_model(model_path):\n",
    "    model = tf.saved_model.load(model_path)\n",
    "    return model\n",
    "\n",
    "# Define model names\n",
    "model_names = ['VGG19', 'VGG16', 'Xception', 'MobileNetV2', 'InceptionV3', 'ResNet50']\n",
    "detection_model_path = \"detection_model.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = load_detection_model(detection_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset location\n",
    "loc = \"/kaggle/input/autism-image-data/AutismDataset/consolidated\"  # Update this path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('output/train', exist_ok=True)\n",
    "os.makedirs('output/val', exist_ok=True)\n",
    "os.makedirs('output/test', exist_ok=True)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "splitfolders.ratio(loc, output=\"output\", seed=0, ratio=(0.80, 0.1, 0.1))\n",
    "train_dir = \"output/train\"\n",
    "test_dir = \"output/test\"\n",
    "val_dir = \"output/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "train_data = image_dataset_from_directory(train_dir, batch_size=batch_size, image_size=image_size, label_mode='categorical', shuffle=True, seed=0)\n",
    "test_data = image_dataset_from_directory(test_dir, batch_size=batch_size, image_size=image_size, label_mode='categorical', shuffle=False, seed=0)\n",
    "val_data = image_dataset_from_directory(val_dir, batch_size=batch_size, image_size=image_size, label_mode='categorical', shuffle=False, seed=0)\n",
    "\n",
    "class_names = train_data.class_names\n",
    "class_count = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name):\n",
    "    input_shape = (224, 224, 3)\n",
    "    base_model = None\n",
    "    \n",
    "    print(f\"Creating model: {model_name}\")  # Debugging statement\n",
    "\n",
    "    if model_name == 'VGG19':\n",
    "        base_model = VGG19(include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling='max')\n",
    "    elif model_name == 'VGG16':\n",
    "        base_model = Xception(include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling='max')\n",
    "    elif model_name == 'Xception':\n",
    "        base_model = Xception(include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling='max')\n",
    "    elif model_name == 'MobileNetV2':\n",
    "        base_model = MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling='max')\n",
    "    elif model_name == 'InceptionV3':\n",
    "        base_model = InceptionV3(include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling='max')\n",
    "    elif model_name == 'ResNet50':\n",
    "        base_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling='max')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")  # Raise an error for unknown model names\n",
    "\n",
    "    if base_model is None:\n",
    "        raise RuntimeError(f\"Failed to create base model for: {model_name}\")  # Ensure base_model is created\n",
    "\n",
    "    # Add custom layers\n",
    "    x = base_model.output\n",
    "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.002)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(rate=0.45, seed=0)(x)\n",
    "    output = Dense(class_count, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(epochs, train_accuracies, val_accuracies, train_losses, val_losses, y_test, y_pred, class_names, model_name):\n",
    "    # Combined Accuracy and Loss plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(epochs), train_accuracies, label='Train Accuracy', color='blue')\n",
    "    plt.plot(range(epochs), val_accuracies, label='Val Accuracy', color='orange')\n",
    "    plt.plot(range(epochs), train_losses, label='Train Loss', linestyle='--', color='green')\n",
    "    plt.plot(range(epochs), val_losses, label='Val Loss', linestyle='--', color='red')\n",
    "    plt.title(f'Accuracy and Loss vs Epochs for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Convert y_test and y_pred to binary format for each class (One-vs-Rest)\n",
    "    y_test_bin = label_binarize(y_test, classes=list(range(len(class_names))))\n",
    "    y_pred_bin = label_binarize(y_pred, classes=list(range(len(class_names))))\n",
    "\n",
    "    # Combined plot of Precision-Recall, F1-Score vs Recall, and ROC Curves in one graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Ensure the number of classes in y_test_bin and y_pred_bin match class_names length\n",
    "    num_classes = len(class_names)\n",
    "    for i in range(num_classes):\n",
    "        # Precision-Recall\n",
    "        if y_test_bin.shape[1] > i and y_pred_bin.shape[1] > i:\n",
    "            precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_pred_bin[:, i])\n",
    "            plt.plot(recall, precision, label=f'Precision-Recall ', linestyle='-')\n",
    "            \n",
    "            # F1-Score vs Recall\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "            plt.plot(recall, f1_scores, label=f'F1-Score vs Recall ', linestyle='--')\n",
    "    \n",
    "    # ROC Curve\n",
    "    for i in range(num_classes):\n",
    "        if y_test_bin.shape[1] > i and y_pred_bin.shape[1] > i:\n",
    "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_bin[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'(ROC , AUC = {roc_auc:.2f})', linestyle='-.')\n",
    "    \n",
    "    # Adding labels, title, and legend\n",
    "    plt.title(f'Combined Precision-Recall, F1-Score vs Recall, and ROC Curve for {model_name}')\n",
    "    plt.xlabel('Recall / FPR (for ROC)')\n",
    "    plt.ylabel('Precision / TPR / F1-Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_data, val_data, test_data, model_name, class_names):\n",
    "    epochs = 50\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Define the callback for saving the best model\n",
    "    checkpoint_cb = ModelCheckpoint(\n",
    "        filepath=f\"{model_name}_best_model.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Loop over epochs manually\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Starting epoch {epoch + 1}/{epochs}\")  # Debugging line\n",
    "        history = model.fit(\n",
    "            train_data,\n",
    "            epochs=1,  # Train for 1 epoch at a time\n",
    "            validation_data=val_data,\n",
    "            verbose=1,\n",
    "            callbacks=[checkpoint_cb]\n",
    "        )\n",
    "        print(f\"Finished epoch {epoch + 1}\")  # Debugging line\n",
    "        train_accuracies.append(history.history['accuracy'][0])\n",
    "        val_accuracies.append(history.history['val_accuracy'][0])\n",
    "        train_losses.append(history.history['loss'][0])\n",
    "        val_losses.append(history.history['val_loss'][0])\n",
    "\n",
    "    # Load the best model\n",
    "    best_model = tf.keras.models.load_model(f\"{model_name}_best_model.keras\")\n",
    "\n",
    "    # Evaluate on the test dataset\n",
    "    test_loss, test_accuracy = best_model.evaluate(test_data)\n",
    "    print(f\"Test Accuracy for {model_name}: {test_accuracy}\")\n",
    "\n",
    "    # Collect test data and labels\n",
    "    test_data_array = []\n",
    "    labels_array = []\n",
    "    for images, labels in test_data:\n",
    "        test_data_array.append(images.numpy())\n",
    "        labels_array.append(labels.numpy())\n",
    "\n",
    "    X_test = np.concatenate(test_data_array, axis=0)\n",
    "    y_test = np.concatenate(labels_array, axis=0)\n",
    "\n",
    "    # Make predictions on the test dataset\n",
    "    y_pred_probs = best_model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(y_test_labels, y_pred, target_names=class_names))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test_labels, y_pred)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, cmap='crest', annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot accuracy and loss graphs\n",
    "    plot_graphs(epochs, train_accuracies, val_accuracies, train_losses, val_losses, y_test_labels, y_pred, class_names, model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_spectrum(model, img_path, class_names):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = tf.keras.applications.vgg19.preprocess_input(img_array)\n",
    "\n",
    "    # Make prediction\n",
    "    preds = model.predict(img_array)\n",
    "    predicted_class = np.argmax(preds)\n",
    "    confidence = preds[0][predicted_class]\n",
    "\n",
    "    prediction_result = {\n",
    "        'class': class_names[predicted_class],\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "    if class_names[predicted_class] == \"Non_Autistic\":\n",
    "        prediction_result['spectrum_severity'] = None\n",
    "    else:\n",
    "        if confidence >= 0.9:\n",
    "            prediction_result['spectrum_severity'] = \"Severe\"\n",
    "        elif confidence >= 0.7:\n",
    "            prediction_result['spectrum_severity'] = \"Moderate\"\n",
    "        else:\n",
    "            prediction_result['spectrum_severity'] = \"Mild\"\n",
    "    \n",
    "    return prediction_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_visualize(detection_model, image_path, model,img_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading image {image_path}\")\n",
    "        return\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for visualization\n",
    "    img_resized = cv2.resize(img_rgb, (512, 512))  # Resize for detection model input\n",
    "    \n",
    "    # Run object detection\n",
    "    boxes, classes, scores = detect_objects(detection_model, img_resized)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    img_with_boxes = draw_boxes(img_resized, boxes, classes, scores)\n",
    "    \n",
    "    # Predict spectrum for each image\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = tf.keras.applications.vgg19.preprocess_input(img_array)\n",
    "    prediction_result = predict_spectrum(model, image_path, class_names)\n",
    "    preds = model.predict(img_array)\n",
    "    predicted_class = np.argmax(preds)\n",
    "    # Visualize the image with bounding boxes\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_with_boxes)\n",
    "    if class_names[predicted_class] == \"Autistic\":\n",
    "        plt.title(f\"Prediction: {prediction_result['class']} at a spectrum level of: {prediction_result['confidence']*100:.2f}%\\n\" +\n",
    "                  f\"Spectrum Severity: {prediction_result['spectrum_severity']}\")\n",
    "    else:\n",
    "         plt.title(f\"Prediction: {prediction_result['class']}\")\n",
    "            \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def detect_objects(detection_model, image_np):\n",
    "    # Convert image to uint8\n",
    "    image_np = (image_np * 255).astype(np.uint8)\n",
    "    \n",
    "    # Create a TensorFlow tensor with the correct shape and dtype\n",
    "    image_np = tf.convert_to_tensor(image_np[tf.newaxis, ...], dtype=tf.uint8)  # Add batch dimension\n",
    "\n",
    "    # Perform detection\n",
    "    results = detection_model(image_np)\n",
    "    \n",
    "    # Extract results\n",
    "    boxes = results['detection_boxes'][0].numpy()\n",
    "    classes = results['detection_classes'][0].numpy()\n",
    "    scores = results['detection_scores'][0].numpy()\n",
    "\n",
    "    return boxes, classes, scores\n",
    "\n",
    "\n",
    "def draw_boxes(img, boxes, classes, scores, threshold=0.5):\n",
    "    h, w, _ = img.shape\n",
    "    for i in range(len(boxes)):\n",
    "        if scores[i] >= threshold:\n",
    "            ymin, xmin, ymax, xmax = boxes[i]\n",
    "            xmin, xmax, ymin, ymax = int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h)\n",
    "            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            label = f\"Class {int(classes[i])}: {scores[i]:.2f}\"\n",
    "            cv2.putText(img, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    return img\n",
    "\n",
    "def predict_spectrum(model, img_path, class_names):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Use the appropriate preprocessing function\n",
    "    img_array = tf.keras.applications.vgg19.preprocess_input(img_array)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    confidence = np.max(predictions[0])\n",
    "    class_label = class_names[predicted_class]\n",
    "    \n",
    "    severity = \"Mild\" if class_label == \"Autistic\" else None\n",
    "    \n",
    "    return {'class': class_label, 'confidence': confidence, 'spectrum_severity': severity}\n",
    "\n",
    "\n",
    "def test_random_images_with_detection(detection_model, test_dir, model, class_names, num_images=12, images_per_row=4):\n",
    "    # List all image files in the directory\n",
    "    images = []\n",
    "    for root, dirs, files in os.walk(test_dir):\n",
    "        for file in files:\n",
    "            images.append(os.path.join(root, file))\n",
    "\n",
    "    # Randomly choose 'num_images' images\n",
    "    random_images = random.sample(images, num_images)\n",
    "\n",
    "    # Calculate the number of rows needed\n",
    "    num_rows = (num_images + images_per_row - 1) // images_per_row\n",
    "\n",
    "    # Create a matplotlib figure with smaller images\n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(12, 3 * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Run detection and visualization for each random image\n",
    "    for i, img_path in enumerate(random_images):\n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Error loading image {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert color from BGR to RGB\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run object detection (optional, if object detection is relevant)\n",
    "        boxes, classes, scores = detect_objects(detection_model, img_rgb)\n",
    "        \n",
    "        # Draw bounding boxes (optional)\n",
    "        img_with_boxes = draw_boxes(img_rgb, boxes, classes, scores)\n",
    "        \n",
    "        # Prediction for spectrum classification\n",
    "        prediction_result = predict_spectrum(model, img_path, class_names)\n",
    "        class_label = prediction_result['class']\n",
    "        severity = prediction_result.get('spectrum_severity', None)\n",
    "        confidence = prediction_result['confidence']\n",
    "\n",
    "        # Set title based on classification result\n",
    "        if class_label == \"Non_Autistic\":\n",
    "            title = f\"{class_label}\"  # No confidence shown for Non_Autistic\n",
    "        else:\n",
    "            title = f\"{class_label} ({confidence:.2f})\"  # Confidence and severity for Autistic images\n",
    "            if severity:\n",
    "                title += f\"\\nSeverity: {severity}\"\n",
    "\n",
    "        # Plot the image\n",
    "        axes[i].imshow(img_with_boxes)\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Hide any remaining empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    model = create_model(model_name)\n",
    "    print(f\"Training and evaluating model: {model_name}\")\n",
    "    train_and_evaluate_model(model, train_data, val_data, test_data, model_name, class_names)\n",
    "\n",
    "    # Test random images with detection\n",
    "    print(f\"Testing random images with detection model: {model_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
